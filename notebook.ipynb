{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import Sparkinit\n",
    "from data_setup.configuracoes import formatar_sql\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, LongType, StringType, DecimalType, BinaryType\n",
    "spark_start = Sparkinit()\n",
    "\n",
    "spark = spark_start.buscar_sessao_spark()\n",
    "\n",
    "print(f\"WebUI SparkJobs: {spark.sparkContext.uiWebUrl}\")\n",
    "spark.getActiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Carregando o dataframe para verificar a estrutura das colunas e tipos de dados\n",
    "dados = spark.read.format(\"parquet\").load(os.path.abspath(r\"C:\\Users\\gustavo.lopes\\Documentos\\GitHub\\desafio_panvel-data_engineer\\datalake\\transient\\VENDAS\"))\n",
    "\n",
    "dados.printSchema()\n",
    "\n",
    "dados.show(n=1, vertical=True)\n",
    "dados.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a tabela temporária e realizando a consulta que depois usaremos para transformar a camada raw\n",
    "dados.createOrReplaceTempView(\"vendas_tmp\")\n",
    "\n",
    "query = formatar_sql(\"\"\" -- 'Tabela Vendas parquet'\n",
    "SELECT \n",
    "    COALESCE(date_format(vt.d_dt_vd, 'yyyy-MM-dd HH:mm:ss'), '') AS data_emissao,\n",
    "    CAST(vt.n_id_fil AS BIGINT) AS codigo_filial,\n",
    "    CAST(vt.n_id_vd_fil AS BIGINT) AS id_venda_filial,\n",
    "    COALESCE(CAST(vt.v_cli_cod AS STRING), '') AS codigo_cliente,\n",
    "    CAST(vt.n_vlr_tot_vd AS DECIMAL(38, 2)) AS valor_total_venda,\n",
    "    CAST(vt.n_vlr_tot_desc AS DECIMAL(38, 2)) AS valor_total_desconto,\n",
    "    CASE \n",
    "        WHEN vt.v_cpn_eml  = 'SIM' THEN True\n",
    "        ELSE False\n",
    "    END AS enviado_email,\n",
    "    COALESCE(CAST(vt.tp_pgt AS STRING), '') AS tipo_pagamento\n",
    "FROM vendas_tmp as vt\"\"\")\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pedidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carregando o dataframe para verificar a estrutura das colunas e tipos de dados\n",
    "dados = spark.read.format(\"parquet\").load(os.path.abspath(r\"C:\\Users\\gustavo.lopes\\Documentos\\GitHub\\desafio_panvel-data_engineer\\datalake\\transient\\PEDIDOS\"))\n",
    "\n",
    "dados.printSchema()\n",
    "\n",
    "dados.show(n=1, vertical=True)\n",
    "dados.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a tabela temporária e realizando a consulta que depois usaremos para transformar a camada raw\n",
    "dados.createOrReplaceTempView(\"pedidos_tmp\")\n",
    "\n",
    "query = formatar_sql(\"\"\"SELECT \n",
    "    CAST(pd.n_id_pdd AS BIGINT) AS id_pedido,\n",
    "    COALESCE(CAST(pd.d_dt_eft_pdd AS DATE), '') AS data_realizacao_pedido,\n",
    "    COALESCE(date_format(pd.d_dt_entr_pdd, 'yyyy-MM-dd HH:mm:ss'), '') AS data_entrega,\n",
    "    CASE \n",
    "        WHEN pd.v_cnl_orig_pdd = 'L' THEN 'Loja'\n",
    "        WHEN pd.v_cnl_orig_pdd = 'A' THEN 'App'\n",
    "        WHEN pd.v_cnl_orig_pdd = 'S' THEN 'Site'\n",
    "    END AS canal_origem_pedido,\n",
    "    COALESCE(CAST(pd.v_uf_entr_pdd AS STRING), '') AS UF_pedido,\n",
    "    COALESCE(CAST(pd.v_lc_ent_pdd AS STRING), '') AS cidade_entrega,\n",
    "    CAST(pd.n_vlr_tot_pdd AS DECIMAL(38,2)) AS valor_total_pedido\n",
    "FROM pedidos_tmp as pd\"\"\")\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pedidos Vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carregando o dataframe para verificar a estrutura das colunas e tipos de dados\n",
    "dados = spark.read.format(\"parquet\").load(os.path.abspath(r\"C:\\Users\\gustavo.lopes\\Documentos\\GitHub\\desafio_panvel-data_engineer\\datalake\\transient\\PEDIDO_VENDA\"))\n",
    "\n",
    "dados.printSchema()\n",
    "\n",
    "dados.show(n=1, vertical=True)\n",
    "dados.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a tabela temporária e realizando a consulta que depois usaremos para transformar a camada raw\n",
    "dados.createOrReplaceTempView(\"pedido_venda_tmp\")\n",
    "\n",
    "query = formatar_sql(\"\"\"SELECT \n",
    "    CAST(pv.n_id_fil AS BIGINT) AS codigo_filial,\n",
    "    CAST(pv.n_id_vd_fil AS BIGINT) AS id_venda_filial,\n",
    "    CAST(pv.n_id_pdd AS BIGINT) AS id_pedido\n",
    "FROM pedido_venda_tmp as pv\"\"\")\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itens Vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carregando o dataframe para verificar a estrutura das colunas e tipos de dados\n",
    "dados = spark.read.format(\"parquet\").load(os.path.abspath(r\"C:\\Users\\gustavo.lopes\\Documentos\\GitHub\\desafio_panvel-data_engineer\\datalake\\transient\\ITENS_VENDAS\"))\n",
    "\n",
    "dados.printSchema()\n",
    "\n",
    "dados.show(n=1, vertical=True)\n",
    "dados.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a tabela temporária e realizando a consulta que depois usaremos para transformar a camada raw\n",
    "dados.createOrReplaceTempView(\"itens_vendas_tmp\")\n",
    "\n",
    "query = formatar_sql(\"\"\"SELECT \n",
    "    CAST(iv.n_id_fil AS BIGINT) AS codigo_filial,\n",
    "    CAST(iv.n_id_vd_fil AS BIGINT) AS id_venda_filial,\n",
    "    CAST(iv.n_id_it AS BIGINT) AS codigo_item_venda,\n",
    "    CASE \n",
    "        WHEN iv.v_rc_elt  = 'SIM' THEN True\n",
    "        ELSE False\n",
    "    END AS com_receita_eletronica,\n",
    "    CASE \n",
    "        WHEN iv.v_it_vd_conv = 'SIM' THEN 'Convênio'\n",
    "        WHEN iv.v_it_vd_conv = 'NAO' AND iv.n_vlr_desc > 0 THEN 'Promoção'\n",
    "        ELSE 'Sem Desconto'\n",
    "    END AS tipo_desconto,\n",
    "    CAST(iv.n_vlr_pis AS DECIMAL(38,2)) AS valor_pis_item,\n",
    "    CAST(iv.n_vlr_vd AS DECIMAL(38,2)) AS valor_final_item,\n",
    "    CAST(iv.n_vlr_desc AS DECIMAL(38,2)) AS valor_desconto_item,\n",
    "    CAST(iv.n_qtd AS BIGINT) AS quantidade_itens\n",
    "FROM itens_vendas_tmp as iv\"\"\")\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endereços Clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carregando o dataframe para verificar a estrutura das colunas e tipos de dados\n",
    "dados = spark.read.format(\"parquet\").load(os.path.abspath(r\"C:\\Users\\gustavo.lopes\\Documentos\\GitHub\\desafio_panvel-data_engineer\\datalake\\transient\\ENDERECOS_CLIENTES\"))\n",
    "\n",
    "dados.printSchema()\n",
    "\n",
    "dados.show(n=1, vertical=True)\n",
    "dados.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a tabela temporária e realizando a consulta que depois usaremos para transformar a camada raw\n",
    "dados.createOrReplaceTempView(\"enderecos_clientes_tmp\")\n",
    "\n",
    "query = formatar_sql(\"\"\" -- 'Tabela Endereços Clientes parquet'\n",
    "SELECT \n",
    "    CAST(ec.v_id_cli AS STRING) AS codigo_cliente,\n",
    "    CAST(ec.n_sq_end AS BIGINT) AS sequencia_endereco_cliente,\n",
    "    COALESCE(date_format(ec.d_dt_exc, 'yyyy-MM-dd HH:mm:ss'), '') AS data_exclusao_endereco,\n",
    "    CAST(ec.v_lcl AS STRING) AS cidade_endereco,\n",
    "    CAST(ec.v_uf AS STRING) AS UF_endereco\n",
    "FROM enderecos_clientes_tmp as ec\"\"\")\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clientes Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"v_id_cli\", StringType(), True),\n",
    "    StructField(\"b_push\", StringType(), True),\n",
    "    StructField(\"b_sms\", StringType(), True),\n",
    "    StructField(\"b_email\", StringType(), True),\n",
    "    StructField(\"b_call\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Carregando o dataframe para verificar a estrutura das colunas e tipos de dados\n",
    "dados = spark.read.option(\"multiline\",\"true\").format(\"json\").load(os.path.abspath(r\"C:\\Users\\gustavo.lopes\\Documentos\\GitHub\\desafio_panvel-data_engineer\\datalake\\transient\\CLIENTES_OPT\"))\n",
    "\n",
    "dados.printSchema()\n",
    "\n",
    "dados.show(n=2, vertical=True)\n",
    "dados.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a tabela temporária e realizando a consulta que depois usaremos para transformar a camada raw\n",
    "dados.createOrReplaceTempView(\"clientes_opt_tmp\")\n",
    "\n",
    "query = formatar_sql(\"\"\" -- 'Tabela Clientes Opt Json'\n",
    "SELECT \n",
    "    CAST(co.v_id_cli AS STRING) AS codigo_cliente,\n",
    "    CASE \n",
    "        WHEN co.b_push  = True THEN 'SIM'\n",
    "        WHEN co.b_push  = False THEN 'NÃO'\n",
    "        ELSE ''\n",
    "    END AS autoriza_notificacao_push,\n",
    "    CASE \n",
    "        WHEN co.b_sms  = True THEN 'SIM'\n",
    "        WHEN co.b_sms  = False THEN 'NÃO'\n",
    "        ELSE ''\n",
    "    END AS autoriza_notificacao_sms,\n",
    "    CASE \n",
    "        WHEN co.b_email  = True THEN 'SIM'\n",
    "        WHEN co.b_email  = False THEN 'NÃO'\n",
    "        ELSE ''\n",
    "    END AS autoriza_notificacao_email,\n",
    "    CASE \n",
    "        WHEN co.b_call  = True THEN 'SIM'\n",
    "        WHEN co.b_call  = False THEN 'NÃO'\n",
    "        ELSE ''\n",
    "    END AS autoriza_notificacao_ligacao\n",
    "FROM clientes_opt_tmp as co\"\"\")\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carregando o dataframe para verificar a estrutura das colunas e tipos de dados\n",
    "dados = spark.read.format(\"parquet\").load(os.path.abspath(os.path.join(os.getcwd(), \"datalake/transient/CLIENTES\")))\n",
    "\n",
    "dados.printSchema()\n",
    "\n",
    "dados.show(n=1, vertical=True)\n",
    "dados.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a tabela temporária e realizando a consulta que depois usaremos para transformar a camada raw\n",
    "dados.createOrReplaceTempView(\"clientes_tmp\")\n",
    "\n",
    "query = formatar_sql(\"\"\" -- 'Tabela Endereços Clientes parquet'\n",
    "SELECT \n",
    "    CAST(c.v_id_cli AS STRING) AS codigo_cliente,\n",
    "    COALESCE(CAST(c.d_dt_nasc AS DATE), '') AS data_nascimento_cliente,\n",
    "    CASE \n",
    "        WHEN c.v_sx_cli  = 'F' THEN 'Feminino'\n",
    "        WHEN c.v_sx_cli  = 'M' THEN 'Masculino'\n",
    "        ELSE '' \n",
    "    END AS genero_biologico_cliente,\n",
    "    CASE \n",
    "        WHEN c.n_est_cvl  = 1 THEN 'Solteiro'\n",
    "        WHEN c.n_est_cvl  = 2 THEN 'Casado'\n",
    "        WHEN c.n_est_cvl  = 3 THEN 'Viúvo'\n",
    "        WHEN c.n_est_cvl  = 4 THEN 'Desquitado'\n",
    "        WHEN c.n_est_cvl  = 5 THEN 'Divorciado'\n",
    "        WHEN c.n_est_cvl  = 6 THEN 'Outros'\n",
    "        ELSE ''\n",
    "    END AS estado_civil_cliente\n",
    "FROM clientes_tmp as c\"\"\")\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" -- 'Tabela Endereços Clientes parquet'\n",
    "SELECT \n",
    "    CAST(c.v_id_cli AS STRING) AS codigo_cliente,\n",
    "    COALESCE(CAST(c.d_dt_nasc AS DATE), '') AS data_nascimento_cliente,\n",
    "    CASE \n",
    "        WHEN c.v_sx_cli  = 'F' THEN 'Feminino'\n",
    "        WHEN c.v_sx_cli  = 'M' THEN 'Masculino'\n",
    "        ELSE '' \n",
    "    END AS genero_biologico_cliente,\n",
    "    CASE \n",
    "        WHEN c.n_est_cvl  = 1 THEN 'Solteiro'\n",
    "        WHEN c.n_est_cvl  = 2 THEN 'Casado'\n",
    "        WHEN c.n_est_cvl  = 3 THEN 'Viúvo'\n",
    "        WHEN c.n_est_cvl  = 4 THEN 'Desquitado'\n",
    "        WHEN c.n_est_cvl  = 5 THEN 'Divorciado'\n",
    "        WHEN c.n_est_cvl  = 6 THEN 'Outros'\n",
    "        ELSE ''\n",
    "    END AS estado_civil_cliente\n",
    "FROM clientes_tmp as c\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# Mapeia e carrega os parâmetros no dicionário json\n",
    "file_json = os.path.abspath(os.path.join(os.getcwd(), \"data_setup/config/spark_jobs.json\"))\n",
    "\n",
    "with open(file=file_json, mode=\"+rt\") as dict_file:\n",
    "    config = json.loads(dict_file.read())\n",
    "    dict_file.close()\n",
    "config[\"transient_config_dict\"][\"clientes\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desafio_panvel-data_engineer-pE6NpJI7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
